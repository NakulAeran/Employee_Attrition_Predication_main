{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dFzREo1SGlqw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score, confusion_matrix, f1_score\n",
        "import warnings\n",
        "from sklearn.exceptions import FitFailedWarning\n",
        "from sklearn.svm import SVR\n",
        "import seaborn as sns\n",
        "warnings.simplefilter('ignore', FitFailedWarning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jxXFaJAbGltN"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('eda_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF1UxXf0Glxj"
      },
      "outputs": [],
      "source": [
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc1LG1s6Glzc"
      },
      "outputs": [],
      "source": [
        "cat_df=df.select_dtypes(include='object')\n",
        "\n",
        "for i in cat_df:\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    sns.catplot(data=df,x=i,kind='count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmhUqdw_xUlI"
      },
      "outputs": [],
      "source": [
        "#plot distributions\n",
        "k=1\n",
        "plt.figure(figsize=(40, 40))\n",
        "for col in df:\n",
        "  if col==\"Attrition\":\n",
        "    continue\n",
        "  yes = df[df['Attrition'] == 'Yes'][col]\n",
        "  no = df[df['Attrition'] == 'No'][col]\n",
        "  plt.subplot(6, 6, k)\n",
        "  plt.hist(yes, bins=25, alpha=0.5, label='yes', color='b')\n",
        "  plt.hist(no, bins=25, alpha=0.5, label='no', color='r')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.title(col)\n",
        "  k+=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cv2P8TYxUnz"
      },
      "outputs": [],
      "source": [
        "f,ax=plt.subplots(figsize=(20,20))\n",
        "sns.heatmap(df.corr(),annot=True,linewidth=.5,fmt='.1f')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KM9sboQCl99"
      },
      "source": [
        "**ADABOOST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io2WCOKs1fAq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score, confusion_matrix, f1_score, roc_curve,auc\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5pdHczK1fC8"
      },
      "outputs": [],
      "source": [
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfPh67M81fFY"
      },
      "outputs": [],
      "source": [
        "categorical_column = ['Attrition', 'BusinessTravel', 'Department','Gender', 'JobRole', 'MaritalStatus', 'OverTime','EducationField']\n",
        "encoder=LabelEncoder()\n",
        "df[categorical_column]=df[categorical_column].apply(encoder.fit_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKGB3pZX1fIk"
      },
      "outputs": [],
      "source": [
        "y=df['Attrition']\n",
        "X=df.drop(['EmployeeCount','Attrition','EmployeeNumber','Over18','StandardHours'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgh4KPFQ1fK3"
      },
      "outputs": [],
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "X_, y = ros.fit_resample(X,y)\n",
        "X = pd.DataFrame(X_,columns=X.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im2rbpXB1fM-"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SgevAK01fO0"
      },
      "outputs": [],
      "source": [
        "def tune_hyperparameters(model,X,y):\n",
        "  param_grid = {\n",
        "      'n_estimators' : [10,50,250,1000],\n",
        "      'learning_rate' : [0.001,0.01,0.1,1.0,10]\n",
        "  }\n",
        "  grid_search = GridSearchCV(model,param_grid=param_grid)\n",
        "  grid_search.fit(X,y)\n",
        "  print(\"Best Params: \",grid_search.best_params_)\n",
        "  return grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDFLIkwa1fRi"
      },
      "outputs": [],
      "source": [
        "decision_tree_parameters = {'criterion': 'entropy', 'max_depth': 6, 'max_features': 0.33334, 'max_leaf_nodes': 50, 'min_samples_leaf': 15, 'min_samples_split': 2, 'random_state': 0}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usv3OvsmDYJg"
      },
      "outputs": [],
      "source": [
        "best_parameters_raw = tune_hyperparameters(AdaBoostClassifier(random_state=0,base_estimator=DecisionTreeClassifier(**decision_tree_parameters)),X_train,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBabxdHFDYMF"
      },
      "outputs": [],
      "source": [
        "def train_predict_evaluate(model,X_train,y_train,X_test):\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy: \",accuracy_score(y_test,y_pred))\n",
        "  print(\"Precision: \",precision_score(y_test,y_pred))\n",
        "  print(\"Recall: \",recall_score(y_test,y_pred))\n",
        "  print(\"F1 Score: \",f1_score(y_test,y_pred))\n",
        "  print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
        "\n",
        "  \n",
        "  fpr,tpr,thresholds = roc_curve(y_test,y_pred)\n",
        "  plt.plot(fpr, tpr,color='green',label='ROC curve (area = %0.2f)' % auc(fpr,tpr))\n",
        "  plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.title(\"ROC Curve\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt9lcojJDYPu"
      },
      "outputs": [],
      "source": [
        "train_predict_evaluate(AdaBoostClassifier(random_state=0,base_estimator=DecisionTreeClassifier(**decision_tree_parameters),**best_parameters_raw),X_train,y_train,X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnCfT592EMw4"
      },
      "outputs": [],
      "source": [
        "def cross_validation(model,X,y):\n",
        "  scores = cross_validate(model, X, y, cv=5,scoring=('accuracy','precision','recall','f1'))\n",
        "\n",
        "  metrics = []\n",
        "  metrics.append(np.mean(scores['test_accuracy']))\n",
        "  metrics.append(np.mean(scores['test_precision']))\n",
        "  metrics.append(np.mean(scores['test_recall']))\n",
        "  metrics.append(np.mean(scores['test_f1']))\n",
        "\n",
        "  print(\"Accuracy: \",metrics[0])\n",
        "  print(\"Precision: \",metrics[1])\n",
        "  print(\"Recall: \",metrics[2])\n",
        "  print(\"F1 Score: \",metrics[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02RlpcJWEM0A"
      },
      "outputs": [],
      "source": [
        "cross_validation(AdaBoostClassifier(random_state=0,base_estimator=DecisionTreeClassifier(**decision_tree_parameters),**best_parameters_raw),X,y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tepfrGfxEM4G"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "from subprocess import check_call\n",
        "from IPython.display import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz, export_text, plot_tree, _tree\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, log_loss\n",
        "import matplotlib.font_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwR0izN1HGG7"
      },
      "outputs": [],
      "source": [
        "dt_train_accuracy = []\n",
        "dt_test_accuracy = []\n",
        "\n",
        "# do GridSearch CV over different values\n",
        "for i in np.arange(1, 15):\n",
        "  param_grid = {'criterion':['entropy','gini'],'max_depth': [i],'max_leaf_nodes':[5, 10, 20, 50, 100],'random_state':[0]}\n",
        "  dt = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid, cv=5)\n",
        "  dt.fit(X_train,y_train)\n",
        "  y_train_pred = dt.predict(X_train)\n",
        "  y_pred = dt.predict(X_test)\n",
        "  s = accuracy_score(y_train, y_train_pred)\n",
        "  dt_train_accuracy.append(s)\n",
        "  dt_test_accuracy.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "# plot graph\n",
        "plt.title(\"AdaBoost Classifier: Train and Test Accuracy vs Max Depth\")\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.plot(np.arange(1,15), dt_train_accuracy, label=\"Training Accuracy\")\n",
        "plt.plot(np.arange(1,15), dt_test_accuracy, label=\"Testing Accuracy\")\n",
        "plt.legend()\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3elXKvv5LDgc"
      },
      "source": [
        "DECISION TREE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il-dSrZxHGJp"
      },
      "outputs": [],
      "source": [
        "pip install dtreeviz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyCToiK9LPq4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder\n",
        "from sklearn.svm import SVR\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import RandomOverSampler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9S12-sbLRC-"
      },
      "outputs": [],
      "source": [
        "df.isna().any().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYf5lvhpLRFw"
      },
      "outputs": [],
      "source": [
        "#Columns with string values\n",
        "categorical_column = ['Attrition', 'BusinessTravel', 'Department', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime','EducationField']\n",
        "encoder=LabelEncoder()\n",
        "df[categorical_column]=df[categorical_column].apply(encoder.fit_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0aHHljxLRIq"
      },
      "outputs": [],
      "source": [
        "Y=df['Attrition']\n",
        "X=df.drop(['EmployeeCount','Attrition','EmployeeNumber','Over18','StandardHours'],axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrXk2IuVLRLf"
      },
      "outputs": [],
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "X_, Y = ros.fit_resample(X,Y)\n",
        "X = pd.DataFrame(X_,columns=X.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwrWMpCfLPtY"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTNbKzbWLPwp"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "from subprocess import check_call\n",
        "from IPython.display import Image\n",
        "from dtreeviz.trees import dtreeviz\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz, export_text, plot_tree, _tree\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, log_loss\n",
        "import matplotlib.font_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S22bkt8YLl4w"
      },
      "outputs": [],
      "source": [
        "def decision_tree_maker(X_train, Y_train, X_test, Y_test, my_depth = 5):\n",
        "\n",
        "  # train model\n",
        "  dt = DecisionTreeClassifier(random_state = 0, max_depth = my_depth, criterion='entropy')\n",
        "  dt.fit(X_train, Y_train)\n",
        "\n",
        "  # test model \n",
        "  Y_pred = dt.predict(X_test)\n",
        "  # metric evaluation\n",
        "  dt_accuracy = accuracy_score(Y_test, Y_pred)\n",
        "  print('Acuuracy = ', dt_accuracy)\n",
        "  dt_f1 = f1_score(Y_test, Y_pred)\n",
        "  print('F1 Score = ', dt_f1)\n",
        "  dt_precision = precision_score(Y_test, Y_pred)\n",
        "  print('Precision = ', dt_precision)\n",
        "  dt_recall = recall_score(Y_test, Y_pred)\n",
        "  print('Recall = ', dt_recall)\n",
        "  dt_confusion_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "  print('Confusion Matrix:\\n', dt_confusion_matrix)\n",
        "\n",
        "  features = ['Age', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome', 'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole',  'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating',  'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears',  'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
        "  classes = ['No', 'Yes']\n",
        "\n",
        "  # text representation of decision tree\n",
        "  text_representation = export_text(dt, feature_names=features)\n",
        "  print('------------ Text Representation of Decision Tree ----------')\n",
        "  print(text_representation)\n",
        "\n",
        "  np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
        "  # graphical represenation of decision tree, save to file 'decision_tree.png'\n",
        "  dot_data = export_graphviz(dt, out_file ='tree.dot', feature_names=features, class_names=classes)\n",
        "  check_call(['dot','-Tpng','tree.dot','-o','decision_tree.png'])\n",
        "\n",
        "  # decision tree detailed visualisation\n",
        "  viz = dtreeviz(dt, X_train, Y_train, target_name=\"Attrition Rate\",feature_names=features, class_names=classes)\n",
        "  return viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkcsYRPdLl8Y"
      },
      "outputs": [],
      "source": [
        "decision_tree_maker(X_train, y_train, X_test, y_test, 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1ftHrnPLl_4"
      },
      "outputs": [],
      "source": [
        "Image('decision_tree.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQD17xuJLmCK"
      },
      "outputs": [],
      "source": [
        "def get_feature_importance(dt, X_train):\n",
        "  # feature importance\n",
        "  # The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.\n",
        "  for importance, name in sorted(zip(dt.feature_importances_, X_train.columns), reverse=True):\n",
        "    print(name, importance)\n",
        "\n",
        "  # plot\n",
        "  plt.xticks(rotation='vertical')\n",
        "  plt.bar(X_train.columns, dt.feature_importances_, align='edge', width=0.3)\n",
        "  plt.xlabel(\"Features\")\n",
        "  plt.ylabel(\"Importance\")\n",
        "  plt.title(\"Feature Importance for the Decision tree\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSumImxvLmD0"
      },
      "outputs": [],
      "source": [
        "dt_train_accuracy = []\n",
        "dt_test_accuracy = []\n",
        "\n",
        "# do GridSearch CV over different values\n",
        "for i in np.arange(1, 15):\n",
        "  param_grid = {'criterion':['entropy','gini'],'max_depth': [i],'max_leaf_nodes':[5, 10, 20, 50, 100],'random_state':[0]}\n",
        "  dt = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid, cv=5)\n",
        "  dt.fit(X_train,y_train)\n",
        "  y_train_pred = dt.predict(X_train)\n",
        "  y_pred = dt.predict(X_test)\n",
        "  s = accuracy_score(y_train, y_train_pred)\n",
        "  dt_train_accuracy.append(s)\n",
        "  dt_test_accuracy.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "# plot graph\n",
        "plt.title(\"Decision Tree Classifier: Train and Test Accuracy vs Max Depth\")\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.plot(np.arange(1,15), dt_train_accuracy, label=\"Training Accuracy\")\n",
        "plt.plot(np.arange(1,15), dt_test_accuracy, label=\"Testing Accuracy\")\n",
        "plt.legend()\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5qCVWbnMTMx"
      },
      "outputs": [],
      "source": [
        "# performing GridSearchCV on data to get best model hyperparameters\n",
        "def gcv(X_train, Y_train, X_test, Y_test):\n",
        "\n",
        "  # make scorer\n",
        "  scoring = make_scorer(accuracy_score)\n",
        "\n",
        "  # define parameters\n",
        "  max_depth = [int(x) for x in np.linspace(2, 15, num=10)] \n",
        "  # max_depth.append(None)\n",
        "\n",
        "  # perform GridSearchCV with given parameters\n",
        "  g_cv = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),\n",
        "                param_grid={'criterion': ['gini', 'entropy'],\n",
        "                            'max_depth': max_depth,\n",
        "                            'max_features': ['auto', 'log2', 'sqrt', 0.33334],\n",
        "                            'min_samples_leaf': [1, 2, 5, 10, 15, 20],\n",
        "                            'min_samples_split': range(2, 10),\n",
        "                            'max_leaf_nodes': [5, 10, 20, 50, 100],\n",
        "                            'random_state': [0]},\n",
        "                scoring=scoring, cv=5, refit=True)\n",
        "\n",
        "  g_cv.fit(X_train, Y_train)\n",
        "\n",
        "  # get best parameter values\n",
        "  print(g_cv.best_params_)\n",
        "\n",
        "  # train new model with best parameters\n",
        "  dt_classifier = DecisionTreeClassifier(**g_cv.best_params_).fit(X_train, Y_train)\n",
        "  Y_pred = dt_classifier.predict(X_test)\n",
        "  dt_accuracy = accuracy_score(Y_test, Y_pred)\n",
        "  print('Accuracy = ', dt_accuracy)\n",
        "  dt_f1 = f1_score(Y_test, Y_pred)\n",
        "  print('F1 Score = ', dt_f1)\n",
        "  dt_precision = precision_score(Y_test, Y_pred)\n",
        "  print('Precision = ', dt_precision)\n",
        "  dt_recall = recall_score(Y_test, Y_pred)\n",
        "  print('Recall = ', dt_recall)\n",
        "  return dt_classifier, g_cv.best_params_, dt_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3DfxAE5MTPa"
      },
      "outputs": [],
      "source": [
        "dt_classifier, best_params_raw, accuracy_cv = gcv(X_train, y_train, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1nl5p6PMTSC"
      },
      "outputs": [],
      "source": [
        "# train new model with best parameters\n",
        "dt_classifier = DecisionTreeClassifier(**best_params_raw).fit(X_train, y_train)\n",
        "Y_pred = dt_classifier.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, Y_pred)\n",
        "print('Accuracy = ', dt_accuracy)\n",
        "dt_f1 = f1_score(y_test, Y_pred)\n",
        "print('F1 Score = ', dt_f1)\n",
        "dt_precision = precision_score(y_test, Y_pred)\n",
        "print('Precision = ', dt_precision)\n",
        "dt_recall = recall_score(y_test, Y_pred)\n",
        "print('Recall = ', dt_recall)\n",
        "features = ['Age', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome', 'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole',  'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating',  'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears',  'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
        "classes = ['No', 'Yes']\n",
        "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
        "# graphical represenation of decision tree, save to file 'decision_tree.png'\n",
        "dot_data = export_graphviz(dt_classifier, out_file ='tree_best.dot', feature_names=features, class_names=classes)\n",
        "check_call(['dot','-Tpng','tree_best.dot','-o','decision_tree_best.png'])\n",
        "dtreeviz(dt_classifier, X_train, y_train, target_name=\"Attrition Rate\",feature_names=features, class_names=classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g5x_nMzMTVK"
      },
      "outputs": [],
      "source": [
        "def cross_validation(model, X, y, my_cv = 5):\n",
        "  scores = cross_validate(model, X, y, cv=my_cv, scoring=('accuracy','precision','recall','f1'))\n",
        "  print(scores)\n",
        "  print(\"Accuracy: \", np.mean(scores['test_accuracy']))\n",
        "  print(\"Precision: \", np.mean(scores['test_precision']))\n",
        "  print(\"Recall: \", np.mean(scores['test_recall']))\n",
        "  print(\"F1 Score: \", np.mean(scores['test_f1']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETfS0QdSMTXo"
      },
      "outputs": [],
      "source": [
        "cross_validation(DecisionTreeClassifier(**best_params_raw), X, Y, 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2nS3jKRVxS-"
      },
      "outputs": [],
      "source": [
        "get_feature_importance(dt_classifier, X_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F05u7npzWTA_"
      },
      "source": [
        "# **KNeighborsClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imgCPgJpWRxS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score, confusion_matrix, f1_score, roc_curve,auc\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1WsDDuNWhoV"
      },
      "outputs": [],
      "source": [
        "categorical_column = ['Attrition', 'BusinessTravel', 'Department','Gender', 'JobRole', 'MaritalStatus', 'OverTime','EducationField']\n",
        "encoder=LabelEncoder()\n",
        "df[categorical_column]=df[categorical_column].apply(encoder.fit_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvOr716RWhqz"
      },
      "outputs": [],
      "source": [
        "y=df['Attrition']\n",
        "X=df.drop(['EmployeeCount','Attrition','EmployeeNumber','Over18','StandardHours'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scU4uAlMWhtp"
      },
      "outputs": [],
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "X_, y = ros.fit_resample(X,y)\n",
        "X = pd.DataFrame(X_,columns=X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBpk_gI6Whv6"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoeyOEGSWhyi"
      },
      "outputs": [],
      "source": [
        "standard_scaler = StandardScaler()\n",
        "\n",
        "X_train_standardized = standard_scaler.fit_transform(X_train)\n",
        "X_test_standardized = standard_scaler.transform(X_test)\n",
        "\n",
        "X_standardized = standard_scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBoN9Yr_Wh1J"
      },
      "outputs": [],
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "X_train_normalized = min_max_scaler.fit_transform(X_train)\n",
        "X_test_normalized = min_max_scaler.transform(X_test)\n",
        "\n",
        "X_normalized = min_max_scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCOu1FZhWR3d"
      },
      "outputs": [],
      "source": [
        "def tune_hyperparameters(model,X,y):\n",
        "  param_grid = {\n",
        "      'n_neighbors' : np.arange(5,20,2),\n",
        "      'leaf_size' : np.arange(1,50,5),\n",
        "      'weights' : ['uniform','distance']\n",
        "  }\n",
        "  grid_search = GridSearchCV(model,param_grid=param_grid)\n",
        "  grid_search.fit(X,y)\n",
        "  print(\"Best Params: \",grid_search.best_params_)\n",
        "  return grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFMKRklyXCw4"
      },
      "outputs": [],
      "source": [
        "best_parameters_raw = tune_hyperparameters(KNeighborsClassifier(),X_train,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVWxSSKxXCz4"
      },
      "outputs": [],
      "source": [
        "best_parameters_std = tune_hyperparameters(KNeighborsClassifier(),X_train_standardized,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcwZhZ0-XC25"
      },
      "outputs": [],
      "source": [
        "best_parameters_norm = tune_hyperparameters(KNeighborsClassifier(),X_train_normalized,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2Tdux_HXC6a"
      },
      "outputs": [],
      "source": [
        "def train_predict_evaluate(model,X_train,y_train,X_test):\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy: \",accuracy_score(y_test,y_pred))\n",
        "  print(\"Precision: \",precision_score(y_test,y_pred))\n",
        "  print(\"Recall: \",recall_score(y_test,y_pred))\n",
        "  print(\"F1 Score: \",f1_score(y_test,y_pred))\n",
        "  print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
        "\n",
        "  \n",
        "  fpr,tpr,thresholds = roc_curve(y_test,y_pred)\n",
        "  plt.plot(fpr, tpr,color='green',label='ROC curve (area = %0.2f)' % auc(fpr,tpr))\n",
        "  plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.title(\"ROC Curve\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt0aAw6tXYXe"
      },
      "outputs": [],
      "source": [
        "train_predict_evaluate(KNeighborsClassifier(**best_parameters_raw),X_train,y_train,X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b6NiflcXYaG"
      },
      "outputs": [],
      "source": [
        "train_predict_evaluate(KNeighborsClassifier(**best_parameters_std),X_train_standardized,y_train,X_test_standardized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVDJ0JrxXYcw"
      },
      "outputs": [],
      "source": [
        "def cross_validation(model,X,y):\n",
        "  scores = cross_validate(model, X, y, cv=5,scoring=('accuracy','precision','recall','f1'))\n",
        "\n",
        "  metrics = []\n",
        "  metrics.append(np.mean(scores['test_accuracy']))\n",
        "  metrics.append(np.mean(scores['test_precision']))\n",
        "  metrics.append(np.mean(scores['test_recall']))\n",
        "  metrics.append(np.mean(scores['test_f1']))\n",
        "\n",
        "  print(\"Accuracy: \",metrics[0])\n",
        "  print(\"Precision: \",metrics[1])\n",
        "  print(\"Recall: \",metrics[2])\n",
        "  print(\"F1 Score: \",metrics[3])\n",
        "\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki5i4lN9XYfJ"
      },
      "outputs": [],
      "source": [
        "metrics = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tj_-zpuX4TL"
      },
      "outputs": [],
      "source": [
        "metrics.append(cross_validation(KNeighborsClassifier(**best_parameters_raw),X,y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVuELojqX4V0"
      },
      "outputs": [],
      "source": [
        "metrics.append(cross_validation(KNeighborsClassifier(**best_parameters_std),X_standardized,y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ija0ckshX4Ya"
      },
      "outputs": [],
      "source": [
        "metrics.append(cross_validation(KNeighborsClassifier(**best_parameters_norm),X_normalized,y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3lm9CxQX4bB"
      },
      "outputs": [],
      "source": [
        "mdf = pd.DataFrame(metrics,columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1 Score\"],index=[\"Without Scaling\",\"With Standardization\",\"With Normalization\"])\n",
        "mdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfJrd_WvYfIr"
      },
      "outputs": [],
      "source": [
        "mdf['Accuracy'].plot()\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMOcTA-RYfL8"
      },
      "outputs": [],
      "source": [
        "mdf['Precision'].plot()\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDEa75Z-YfOq"
      },
      "outputs": [],
      "source": [
        "mdf['Recall'].plot()\n",
        "plt.ylabel(\"Recall\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgjTVLlsYfRr"
      },
      "outputs": [],
      "source": [
        "mdf['F1 Score'].plot()\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T9--1g-aZ1R"
      },
      "source": [
        "**Linear** **Discrimant** **Analysis** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1hYn3zCaIAA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score, confusion_matrix, f1_score, roc_curve,auc\n",
        "import warnings\n",
        "from sklearn.exceptions import FitFailedWarning\n",
        "warnings.simplefilter('ignore', FitFailedWarning)\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2rd_haHaICi"
      },
      "outputs": [],
      "source": [
        "categorical_column = ['Attrition', 'BusinessTravel', 'Department','Gender', 'JobRole', 'MaritalStatus', 'OverTime','EducationField']\n",
        "encoder=LabelEncoder()\n",
        "df[categorical_column]=df[categorical_column].apply(encoder.fit_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN_6nCgbaIFN"
      },
      "outputs": [],
      "source": [
        "y=df['Attrition']\n",
        "X=df.drop(['EmployeeCount','Attrition','EmployeeNumber','Over18','StandardHours'],axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO6ak7LgaIIT"
      },
      "outputs": [],
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "X_, y = ros.fit_resample(X,y)\n",
        "X = pd.DataFrame(X_,columns=X.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UVtMubeaIK-"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPgi61LKaION"
      },
      "outputs": [],
      "source": [
        "def tune_hyperparameters(model,X,y):\n",
        "  param_grid = {\"solver\":['svd', 'lsqr', 'eigen'], \"shrinkage\":['auto',0.1,0.01,1,0]}\n",
        "  grid_search = GridSearchCV(model,param_grid=param_grid,scoring='accuracy')\n",
        "  grid_search.fit(X,y)\n",
        "  print(\"Best Params: \",grid_search.best_params_)\n",
        "  return grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyCwasTQaxUy"
      },
      "outputs": [],
      "source": [
        "best_params = tune_hyperparameters(LinearDiscriminantAnalysis(),X_train,y_train)\n",
        "best_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFWNqex3axXW"
      },
      "outputs": [],
      "source": [
        "def train_predict_evaluate(model,X_train,y_train,X_test):\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy: \",accuracy_score(y_test,y_pred))\n",
        "  print(\"Precision: \",precision_score(y_test,y_pred,zero_division=0))\n",
        "  print(\"Recall: \",recall_score(y_test,y_pred,zero_division=0))\n",
        "  print(\"F1 Score: \",f1_score(y_test,y_pred,zero_division=0))\n",
        "  print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
        "\n",
        "  \n",
        "  fpr,tpr,thresholds = roc_curve(y_test,y_pred)\n",
        "  plt.plot(fpr, tpr,color='green',label='ROC curve (area = %0.2f)' % auc(fpr,tpr))\n",
        "  plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.title(\"ROC Curve\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3EN87LiaxZ6"
      },
      "outputs": [],
      "source": [
        "train_predict_evaluate(LinearDiscriminantAnalysis(shrinkage='auto', solver='lsqr'),X_train,y_train,X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoPf5evLaxc3"
      },
      "outputs": [],
      "source": [
        "def cross_validation(model,X,y):\n",
        "  scores = cross_validate(model, X, y, cv=5,scoring=('accuracy','precision','recall','f1'))\n",
        "\n",
        "  metrics = []\n",
        "  metrics.append(np.mean(scores['test_accuracy']))\n",
        "  metrics.append(np.mean(scores['test_precision']))\n",
        "  metrics.append(np.mean(scores['test_recall']))\n",
        "  metrics.append(np.mean(scores['test_f1']))\n",
        "\n",
        "  print(\"Accuracy: \",metrics[0])\n",
        "  print(\"Precision: \",metrics[1])\n",
        "  print(\"Recall: \",metrics[2])\n",
        "  print(\"F1 Score: \",metrics[3])\n",
        "\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQq5g2DZaxfj"
      },
      "outputs": [],
      "source": [
        "metrics = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPQO4nB7axib"
      },
      "outputs": [],
      "source": [
        "metrics.append(cross_validation(LinearDiscriminantAnalysis(**best_params),X,y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbJfxjE5bh4w"
      },
      "source": [
        "**MLP Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AldagccbjAN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score, confusion_matrix, f1_score, roc_curve,auc\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9inW70x_bjDF"
      },
      "outputs": [],
      "source": [
        "standard_scaler = StandardScaler()\n",
        "\n",
        "X_train_standardized = standard_scaler.fit_transform(X_train)\n",
        "X_test_standardized = standard_scaler.transform(X_test)\n",
        "\n",
        "X_standardized = standard_scaler.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s4Vr5-ebjF_"
      },
      "outputs": [],
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "X_train_normalized = min_max_scaler.fit_transform(X_train)\n",
        "X_test_normalized = min_max_scaler.transform(X_test)\n",
        "\n",
        "X_normalized = min_max_scaler.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP_D4sVicko2"
      },
      "outputs": [],
      "source": [
        "def tune_hyperparameters(model,X,y):\n",
        "  param_grid = {\n",
        "      'activation' : ['identity','logistic','tanh','relu'],\n",
        "      'solver': ['lbfgs','sgd','adam'],\n",
        "      'alpha': [0.0001,0.05]\n",
        "  }\n",
        "  grid_search = GridSearchCV(model,param_grid=param_grid)\n",
        "  grid_search.fit(X,y)\n",
        "  print(\"Best Params: \",grid_search.best_params_)\n",
        "  return grid_search.best_params_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUMETH3Hqd_P"
      },
      "outputs": [],
      "source": [
        "def train_predict_evaluate(model,X_train,y_train,X_test):\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy: \",accuracy_score(y_test,y_pred))\n",
        "  print(\"Precision: \",precision_score(y_test,y_pred))\n",
        "  print(\"Recall: \",recall_score(y_test,y_pred))\n",
        "  print(\"F1 Score: \",f1_score(y_test,y_pred))\n",
        "  print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
        "\n",
        "  \n",
        "  fpr,tpr,thresholds = roc_curve(y_test,y_pred)\n",
        "  plt.plot(fpr, tpr,color='green',label='ROC curve (area = %0.2f)' % auc(fpr,tpr))\n",
        "  plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.title(\"ROC Curve\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8ZRHCcGqeBv"
      },
      "outputs": [],
      "source": [
        "train_predict_evaluate(MLPClassifier(max_iter=100000,random_state=0),X_train,y_train,X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqCePxYPvybj"
      },
      "outputs": [],
      "source": [
        "def cross_validation(model,X,y):\n",
        "  scores = cross_validate(model, X, y, cv=5,scoring=('accuracy','precision','recall','f1'))\n",
        "\n",
        "  metrics = []\n",
        "  metrics.append(np.mean(scores['test_accuracy']))\n",
        "  metrics.append(np.mean(scores['test_precision']))\n",
        "  metrics.append(np.mean(scores['test_recall']))\n",
        "  metrics.append(np.mean(scores['test_f1']))\n",
        "\n",
        "  print(\"Accuracy: \",metrics[0])\n",
        "  print(\"Precision: \",metrics[1])\n",
        "  print(\"Recall: \",metrics[2])\n",
        "  print(\"F1 Score: \",metrics[3])\n",
        "\n",
        "  return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08XB0Epkvyei"
      },
      "outputs": [],
      "source": [
        "metrics = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQNhElFgvyhc"
      },
      "outputs": [],
      "source": [
        "metrics.append(cross_validation(MLPClassifier(max_iter=100000,random_state=0),X,y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zh-FknpwQy1"
      },
      "source": [
        "**RandomForest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osm6KnSBwXw3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score, confusion_matrix, f1_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import warnings\n",
        "from sklearn.exceptions import FitFailedWarning\n",
        "warnings.simplefilter('ignore', FitFailedWarning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeUqae6MwXz_"
      },
      "outputs": [],
      "source": [
        "df.isna().any().any()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9g4yDnawX2Z"
      },
      "outputs": [],
      "source": [
        "def train_predict_evaluate(model,X_train,y_train,X_test):\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  print(\"Accuracy: \",accuracy_score(y_test,y_pred))\n",
        "  print(\"Precision: \",precision_score(y_test,y_pred))\n",
        "  print(\"Recall: \",recall_score(y_test,y_pred))\n",
        "  print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBw3Og-6wX5J"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "# Setting random state for randomization\n",
        "rf = RandomForestClassifier(random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76vuiGL_wX_q"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Taking a huge range of values to get nearer to optimal parameters\n",
        "\n",
        "# Parameters to check\n",
        "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
        "max_features = ['auto', 'sqrt']\n",
        "max_depth = [int(x) for x in np.linspace(10, 150, num = 10)]\n",
        "max_depth.append(None)\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "\n",
        "# Creating random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ9uFM8gwYC7"
      },
      "outputs": [],
      "source": [
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n",
        "                               n_iter = 100, cv = 3, verbose=2, \n",
        "                               random_state=42, n_jobs = -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKKRDxZbwYFZ"
      },
      "outputs": [],
      "source": [
        "train_predict_evaluate (rf_random, X_train, y_train, X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chqmRpgawYIL"
      },
      "outputs": [],
      "source": [
        "rf_random.best_params_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DLtkVYUywpQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# After getting near to optimal values, using Random Search CV, \n",
        "# we now check for close values using Grid Search CV\n",
        "\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "    'bootstrap': [False],\n",
        "    'max_depth': [100, 105, 110, 115],\n",
        "    'min_samples_leaf': [1,2,3],\n",
        "    'min_samples_split': [2,3,4],\n",
        "    'n_estimators': [250,300,350,400]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmOMpChcywdj"
      },
      "outputs": [],
      "source": [
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
        "                          cv = 3, n_jobs = -1, verbose = 2)\n",
        "\n",
        "# Results on the Grid Search model\n",
        "train_predict_evaluate (grid_search, X_train, y_train, X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU-zFeWhywiN"
      },
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko7OUeeYywmm"
      },
      "outputs": [],
      "source": [
        "best_rf = RandomForestClassifier(bootstrap=False, max_depth=100, \n",
        "                                 min_samples_leaf=1,min_samples_split=2, \n",
        "                                 n_estimators=250, random_state=42)\n",
        "train_predict_evaluate (best_rf, X_train, y_train, X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIbP8OlZ1Vx-"
      },
      "outputs": [],
      "source": [
        "def cross_validation(model,X,y):\n",
        "  scores = cross_validate(model, X, y, cv=5,scoring=('accuracy','precision','recall','f1'))\n",
        "\n",
        "  print(\"Accuracy: \", np.mean(scores['test_accuracy']))\n",
        "  print(\"Precision: \", np.mean(scores['test_precision']))\n",
        "  print(\"Recall: \", np.mean(scores['test_recall']))\n",
        "  print(\"F1 Score: \", np.mean(scores['test_f1']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5TPZWb-1V1D"
      },
      "outputs": [],
      "source": [
        "cross_validation(rf, X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcCCZphw1V3x"
      },
      "outputs": [],
      "source": [
        "cross_validation(best_rf, X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JEe91my1V9Z"
      },
      "outputs": [],
      "source": [
        "feature_imp = pd.Series(best_rf.feature_importances_, index=list(X.columns)).sort_values(ascending=False)\n",
        "print(feature_imp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo6S7EPY1V67"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Creating a bar plot using sns\n",
        "sns.set(rc={'figure.figsize':(13,9)})\n",
        "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
        "\n",
        "# Adding labels\n",
        "plt.xlabel('Feature Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.title(\"Important Features\")\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFtc_CA4Y630"
      },
      "source": [
        "**SVC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9_Y7Y5R2h-I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score, confusion_matrix, f1_score, roc_curve,auc\n",
        "from imblearn.over_sampling import RandomOverSampler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp1AEVj_2iIg"
      },
      "outputs": [],
      "source": [
        "standard_scaler = StandardScaler()\n",
        "\n",
        "X_train_standardized = standard_scaler.fit_transform(X_train)\n",
        "X_test_standardized = standard_scaler.transform(X_test)\n",
        "\n",
        "X_standardized = standard_scaler.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKjPUY_g2iL2"
      },
      "outputs": [],
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "X_train_normalized = min_max_scaler.fit_transform(X_train)\n",
        "X_test_normalized = min_max_scaler.transform(X_test)\n",
        "\n",
        "X_normalized = min_max_scaler.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mflq6UdZ2iOY"
      },
      "outputs": [],
      "source": [
        "def tune_hyperparameters(model,X,y):\n",
        "  param_grid = {\n",
        "      'C' : [0.1,1,10,100,1000],\n",
        "      'kernel' : ['linear','rbf']\n",
        "  }\n",
        "\n",
        "  grid_search = GridSearchCV(model,param_grid=param_grid)\n",
        "  grid_search.fit(X,y)\n",
        "  print(\"Best Params: \",grid_search.best_params_)\n",
        "  return grid_search.best_params_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D865clV2iRe"
      },
      "outputs": [],
      "source": [
        "best_parameters_std = tune_hyperparameters(SVC(random_state=0),X_train_standardized,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldayRC9i2iUZ"
      },
      "outputs": [],
      "source": [
        "best_parameters_norm = tune_hyperparameters(SVC(random_state=0),X_train_normalized,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE_Py_ub2iXR"
      },
      "outputs": [],
      "source": [
        "def train_predict_evaluate(model,X_train,y_train,X_test):\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy: \",accuracy_score(y_test,y_pred))\n",
        "  print(\"Precision: \",precision_score(y_test,y_pred))\n",
        "  print(\"Recall: \",recall_score(y_test,y_pred))\n",
        "  print(\"F1 Score: \",f1_score(y_test,y_pred))\n",
        "  print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
        "\n",
        "  \n",
        "  fpr,tpr,thresholds = roc_curve(y_test,y_pred)\n",
        "  plt.plot(fpr, tpr,color='green',label='ROC curve (area = %0.2f)' % auc(fpr,tpr))\n",
        "  plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.title(\"ROC Curve\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V7JKWPy7Dps"
      },
      "outputs": [],
      "source": [
        "train_predict_evaluate(SVC(random_state=0),X_train,y_train,X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-V0HiX37DsZ"
      },
      "outputs": [],
      "source": [
        "train_predict_evaluate(SVC(random_state=0,**best_parameters_std),X_train_standardized,y_train,X_test_standardized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PON2VJBA7DvG"
      },
      "outputs": [],
      "source": [
        "train_predict_evaluate(SVC(random_state=0,**best_parameters_norm),X_train_normalized,y_train,X_test_normalized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDE0bdVK7Dxz"
      },
      "outputs": [],
      "source": [
        "def cross_validation(model,X,y):\n",
        "  scores = cross_validate(model, X, y, cv=5,scoring=('accuracy','precision','recall','f1'))\n",
        "\n",
        "  metrics = []\n",
        "  metrics.append(np.mean(scores['test_accuracy']))\n",
        "  metrics.append(np.mean(scores['test_precision']))\n",
        "  metrics.append(np.mean(scores['test_recall']))\n",
        "  metrics.append(np.mean(scores['test_f1']))\n",
        "\n",
        "  print(\"Accuracy: \",metrics[0])\n",
        "  print(\"Precision: \",metrics[1])\n",
        "  print(\"Recall: \",metrics[2])\n",
        "  print(\"F1 Score: \",metrics[3])\n",
        "\n",
        "  return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPJECp7-7TeT"
      },
      "outputs": [],
      "source": [
        "metrics = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXlyAd0_7ThT"
      },
      "outputs": [],
      "source": [
        "metrics.append(cross_validation(SVC(random_state=0),X,y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjylOGcv7TkY"
      },
      "outputs": [],
      "source": [
        "metrics.append(cross_validation(SVC(random_state=0,**best_parameters_std),X_standardized,y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8qqon5S7Z4N"
      },
      "outputs": [],
      "source": [
        "metrics.append(cross_validation(SVC(random_state=0,**best_parameters_norm),X_normalized,y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73SQB1Rp7Z7T"
      },
      "outputs": [],
      "source": [
        "mdf = pd.DataFrame(metrics,columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1 Score\"],index=[\"Without Scaling\",\"With Standardization\",\"With Normalization\"])\n",
        "mdf.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahi54nhj7Z-f"
      },
      "outputs": [],
      "source": [
        "mdf['Accuracy'].plot()\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aah0KxaE7gYC"
      },
      "outputs": [],
      "source": [
        "mdf['Precision'].plot()\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYzJKmpG7ga5"
      },
      "outputs": [],
      "source": [
        "mdf['Recall'].plot()\n",
        "plt.ylabel(\"Recall\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQeXpi-27gdw"
      },
      "outputs": [],
      "source": [
        "mdf['F1 Score'].plot()\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2Dp8tTacBm4"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
